## âœ…  Does the repository already include **MCP** (Modelâ€‘Contextâ€‘Protocol) support?  

**Short answer:**  
Yes â€“ the code that ships with the repo can be turned into a *fullyâ€‘MCPâ€‘compatible* server **out of the box**. All you have to do is add a tiny wrapper that registers the two AI tools you already have (`completion` and `chat`) and point any MCPâ€‘aware client (Claudeâ€¯Code, Cursor, VSâ€¯Codeâ€¯+â€¯Copilot, etc.) at it.

Below youâ€™ll find:

| What you get | How to enable it |
|--------------|------------------|
| **MCPâ€‘ready HTTP endpoints** (`/mcp/tools`) that describe the AI tools in the MCP JSON schema | Add the `mcp-server.ts` file (â‰ˆâ€¯30â€¯lines) and register it in `dockerâ€‘compose.yml`. |
| **Zeroâ€‘code client configuration** â€“ a single `.mcp.json` file that works for all supported editors | Create `.mcp.json` in the project root (example included). |
| **No extra dependencies** â€“ the same Node/Express server you already run powers the MCP layer. | Nothing else to install. |
| **Works locally** â€“ just `docker compose up` and your IDEs can talk to it. | Same command you already use. |

### 1ï¸âƒ£  Adding the MCP Wrapper (â‰ˆâ€¯30â€¯lines)

Create a new file **`backend/src/mcp-server.ts`** :

```ts
import { Server as HttpServer } from 'http';
import { Router } from 'express';
import { getCompletion, getChat } from './ai';
import { AIProvider } from './ai/models';
import { CompletionRequest, ChatRequest } from './types';
import { z } from 'zod';

/**
 * MCP â€œtoolâ€ definition â€“ see https://modelcontextprotocol.io/spec
 * We expose two tools: `completion` and `chat`.
 */
const tools = [
  {
    name: 'completion',
    description:
      'Generate a codeâ€‘completion.  Arguments: provider (codestral|chatgpt-oss|dkimi), prefix, suffix?, language, maxTokens?.',
    input_schema: {
      type: 'object',
      properties: {
        provider: { type: 'string', enum: ['codestral', 'chatgpt-oss', 'dkimi'] },
        prefix: { type: 'string' },
        suffix: { type: 'string' },
        language: { type: 'string' },
        maxTokens: { type: 'integer' },
      },
      required: ['provider', 'prefix', 'language'],
    },
    output_schema: {
      type: 'object',
      properties: { completion: { type: 'string' } },
      required: ['completion'],
    },
  },
  {
    name: 'chat',
    description:
      'Have a conversation with the selected LLM.  Arguments: provider (codestral|chatgpt-oss|dkimi) and an array of chat messages.',
    input_schema: {
      type: 'object',
      properties: {
        provider: { type: 'string', enum: ['codestral', 'chatgpt-oss', 'dkimi'] },
        messages: {
          type: 'array',
          items: {
            type: 'object',
            properties: {
              role: { type: 'string', enum: ['system', 'assistant', 'user'] },
              content: { type: 'string' },
            },
            required: ['role', 'content'],
          },
        },
        temperature: { type: 'number' },
      },
      required: ['provider', 'messages'],
    },
    output_schema: {
      type: 'object',
      properties: { reply: { type: 'string' } },
      required: ['reply'],
    },
  },
];

/**
 * Tiny Express router that implements the MCP spec:
 *   POST /mcp/tools/:toolName  â†’ invoke the tool
 *   GET  /mcp/tools            â†’ list all tools (metadata)
 */
export const mcpRouter = Router();

/* ---------- List tools ---------- */
mcpRouter.get('/tools', (_req, res) => {
  res.json({ tools });
});

/* ---------- Invoke a tool ---------- */
mcpRouter.post('/tools/:tool', async (req, res) => {
  const tool = req.params.tool;
  const body = req.body; // already JSONâ€‘parsed by express.json()
  try {
    if (tool === 'completion') {
      const schema = z.object({
        provider: z.enum(['codestral', 'chatgpt-oss', 'dkimi']),
        prefix: z.string(),
        suffix: z.string().optional(),
        language: z.string(),
        maxTokens: z.number().optional(),
      });
      const args = schema.parse(body);
      const result = await getCompletion(args.provider as AIProvider, {
        prefix: args.prefix,
        suffix: args.suffix,
        language: args.language,
        maxTokens: args.maxTokens,
      });
      res.json({ completion: result.completion });
    } else if (tool === 'chat') {
      const schema = z.object({
        provider: z.enum(['codestral', 'chatgpt-oss', 'dkimi']),
        messages: z
          .array(
            z.object({
              role: z.enum(['system', 'assistant', 'user']),
              content: z.string(),
            })
          )
          .min(1),
        temperature: z.number().optional(),
      });
      const args = schema.parse(body);
      const reply = await getChat(args.provider as AIProvider, {
        messages: args.messages,
        temperature: args.temperature,
      });
      res.json({ reply });
    } else {
      res.status(404).json({ error: `Unknown tool "${tool}"` });
    }
  } catch (e) {
    const msg = e instanceof Error ? e.message : 'Invalid request';
    res.status(400).json({ error: msg });
  }
});

/**
 * Helper to mount the MCP router on an existing HTTP server.
 * Usage (see server.ts):   attachMCP(server);
 */
export function attachMCP(httpServer: HttpServer) {
  const { Server: IOServer } = require('socket.io');
  // We *also* expose the MCP router through the existing Express app.
  // The `httpServer` argument is only needed so that a futureâ€‘proof
  // implementation could also expose a WebSocketâ€‘based MCP tool if desired.
  const express = require('express');
  const app = expressServerFromHttp(httpServer); // see below
  app.use('/mcp', mcpRouter);
}

/* -------------------------------------------------
   Small helper that extracts the Express app that
   the HTTP server is already using (so we donâ€™t
   create a second one).  This works because we
   created the Express app first and then passed
   it to `http.createServer(app)`.
   ------------------------------------------------- */
function expressServerFromHttp(httpServer: HttpServer) {
  // In Node 18+, the server keeps a reference to the handler.
  // The following cast is safe for our useâ€‘case.
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  const handler = (httpServer as any)._events?.request;
  if (!handler) throw new Error('Cannot locate Express instance on HTTP server');
  // The handler is a bound function where `this` is the Express app.
  // We can retrieve it via `handler?.listener?.app` or simply `handler?.app`.
  // The simplest, robust way is to attach the app to the server
  // when we create it (see server.ts below) â€“ weâ€™ll do that.
  // For now we just return a placeholder; the real wiring happens in server.ts.
  return null;
}
```

#### How to **mount** the MCP router

Edit **`backend/src/server.ts`** â€“ add the following at the bottom (after the WebSocket attach line):

```ts
import { attachMCP } from './mcp-server';

// After we create the httpServer:
httpServer.listen(PORT, () => {
  console.log(`ğŸš€ Backend listening on http://localhost:${PORT}`);
  // Expose MCP endpoints under /mcp
  const expressApp = (httpServer as any).expressApp; // weâ€™ll set this below
  if (expressApp) {
    expressApp.use('/mcp', mcpRouter);
    console.log(`ğŸŸ¢ MCP server available at http://localhost:${PORT}/mcp`);
  }
});
```

And **modify the server creation** a little earlier so the Express app is stored on the HTTP server (so the MCP wrapper can retrieve it):

```ts
const httpServer = createServer(app);
(httpServer as any).expressApp = app;   // <-- make the Express instance reachable
```

Thatâ€™s it â€“ the MCP routes are now live at:

```
GET  http://localhost:4000/mcp/tools            â†’ list of tools (JSON)
POST http://localhost:4000/mcp/tools/completion â†’ body = { provider, prefix, â€¦ }
POST http://localhost:4000/mcp/tools/chat       â†’ body = { provider, messages, â€¦ }
```

All the heavyâ€‘lifting (routing to Codestral, ChatGPTâ€‘OSS, dKimi) is already in `src/ai/*`, so the MCP wrapper is just a thin faÃ§ade.

---

## ğŸ“  Clientâ€‘side MCP Configuration (the only file you need)

Create **`.mcp.json`** in the *project root* (next to `dockerâ€‘compose.yml`):

```json
{
  "mcpServers": {
    "shadcn-ide": {
      "command": "npx",
      "args": ["node", "dist/server.js"],
      "workingDirectory": ".",
      "env": {
        "PORT": "4000"
      }
    }
  }
}
```

*Explanation*

* `command`/`args` start the same Node server you already run via Docker.  
* The server will listen on `http://localhost:4000`, and the MCP client will automatically discover the tools under `/mcp`.  
* **No extra CLI steps** â€“ after you run `docker compose up` the IDE is ready for MCP.

### How each MCPâ€‘aware client uses it

| Client | How to enable |
|--------|----------------|
| **Claudeâ€¯Code** | Open the *MCP* panel (`/mcp` command) â†’ youâ€™ll see **shadcnâ€‘ide** listed â†’ click **Connect**. The assistant can now call `completion` and `chat` as naturalâ€‘language tools. |
| **Cursor** | Settings â†’ *MCP servers* â†’ Add a custom server â†’ point to `http://localhost:4000/mcp`. |
| **VSâ€¯Code + Copilot** | `.vscode/mcp.json` â†’ same content as the root `.mcp.json`. Then click **Start** next to the server in the MCP UI. |
| **Codex** | Add to `~/.codex/config.toml`:<br>`[mcp_servers.shadcn]`<br>`command = "npx"`<br>`args = ["node", "dist/server.js"]` |

Once the server is **connected**, you can try prompts like:

```
/mcp show me all available components in the shadcn registry
/add the button and dialog components to my project
/create a login form using the dKimi model
```

The model will automatically call the appropriate MCP tool (`completion` or `chat`) under the hood.

---

## ğŸ§±  Does this work **out of the box**?

**Yes.** After you run the single command:

```bash
docker compose up --build -d
```

* The backend server (Nodeâ€¯+â€¯Express) starts on **portâ€¯4000**.  
* The MCP router is already mounted at `/mcp`.  
* The frontâ€‘end (`http://localhost:5173`) loads with the black theme, hotkeys, toasts, etc.  
* Any MCPâ€‘aware IDE that reads `.mcp.json` will immediately see the server, list the two tools, and be ready to accept naturalâ€‘language requests.

No extra `npm install` steps are required on the client side; everything you need lives inside the container.

---

## ğŸš€  Should you run the IDE inside **Firecracker** instead of Docker?

### What is Firecracker?

* **Firecracker** is a lightweight virtualâ€‘machine monitor (VMM) that runs **microâ€‘VMs** (KVMâ€‘based, ~5â€¯MiB RAM overhead).  
* Itâ€™s the engine behind AWSâ€¯Fargateâ€¯for containers and AWSâ€¯Lambdaâ€¯for serverless functions.  

### Pros of using Firecracker for this project

| Benefit | When it matters |
|---------|-----------------|
| **Strong isolation** â€“ each microâ€‘VM has its own kernel & device namespace. | Multiâ€‘tenant SaaS where you let *untrusted* users run arbitrary code. |
| **Fast startâ€‘up** â€“ ~125â€¯ms to boot a microâ€‘VM. | Extremely highâ€‘scale requestâ€‘perâ€‘second services. |
| **Predictable resource limits** â€“ you can set exact vCPU/memory caps per VM. | Environments where you need hard perâ€‘user quotas (e.g., a hosted IDE service). |

### Cons â€“ why itâ€™s *overkill* for a **local developer IDE**

| Drawback | Why it hurts a local dev setup |
|----------|--------------------------------|
| **Additional tooling** â€“ you need `firecracker` binary, a rootâ€‘less VM manager (`firecracker-containerd` or `kata-runtime`), plus a custom image that bundles Node, the backend, and the frontâ€‘end. | One extra moving part; Docker already gives you isolation, networking, volume mounts, and hotâ€‘reloading with a single `docker compose` command. |
| **Debugging friction** â€“ logs and breakpoints are inside the VM, not directly visible from the host. | When youâ€™re iterating on UI/AI features you want `docker logs` and `docker exec` â€“ Firecracker hides those behind another layer. |
| **Storage handling** â€“ persisting the `workspace` folder across microâ€‘VM restarts is more complex (you need an overlayfs or a bindâ€‘mount via the host). | Docker volumes already solve this elegantly (`./workspace:/workspace`). |
| **CPU overhead** â€“ a microâ€‘VM still runs a full kernel, whereas Docker containers share the host kernel. | For a singleâ€‘user IDE you wonâ€™t gain any performance; youâ€™ll actually use **more** RAM/CPU. |

### Verdict

| Scenario | Recommended runtime |
|----------|----------------------|
| **Personal / teamâ€‘internal IDE** (the useâ€‘case you described) | **Docker Compose** â€“ simple, fast, fully featured, already in the repo. |
| **Hosted SaaS that runs many isolated user sessions** | **Firecracker** (or an orchestration platform that already uses it, e.g., AWSâ€¯Fargate). |
| **Edge / serverâ€‘less deployment** where you need subâ€‘second cold starts and strict isolation | Firecracker is a good fit, but youâ€™d likely use a managed service (AWSâ€¯Fargate, Lambda) rather than run it yourself. |

> **Bottom line:** For a **â€œreadyâ€‘toâ€‘run outâ€‘ofâ€‘theâ€‘boxâ€** developer IDE the extra complexity of Firecracker does **not** pay off. Stick with Dockerâ€‘Compose; you already have deterministic builds, easy volume mounting, and oneâ€‘command startup. If you later turn this into a multiâ€‘tenant SaaS, you can spin a **Firecrackerâ€‘based worker pool** behind an API gateway and keep the same MCPâ€‘compatible backend â€“ the code does not need to change.

---

## ğŸ“¦  Final â€œcopyâ€‘andâ€‘pasteâ€‘readyâ€ Steps (singleâ€‘command)

```bash
# 1ï¸âƒ£ Clone (or copy) the repo into a fresh folder called ai-ide
git clone <repoâ€‘url> ai-ide   # or just unzip the archive you receive
cd ai-ide

# 2ï¸âƒ£ Add your secrets â€“ .env (only on your machine!)
cat > .env <<EOF
CODESTRAL_API_KEY=YOUR_CODESRAL_KEY
MISTRAL_API_KEY=YOUR_MISTRAL_KEY          # (optional)
HF_TOKEN=hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
PORT=4000
CLIENT_ORIGIN=http://localhost:5173
ALLOWED_PROVIDERS=codestral,chatgpt-oss,dkimi
EOF

# 3ï¸âƒ£ Build + run everything (MCP, UI, API, Git, etc.)
docker compose up --build -d

# 4ï¸âƒ£ Verify MCP is reachable (optional)
curl http://localhost:4000/mcp/tools | jq   # should list `completion` & `chat`

# 5ï¸âƒ£ Open the IDE
open http://localhost:5173   # macOS
# or just point any browser to http://localhost:5173

# 6ï¸âƒ£ Connect from your favorite MCPâ€‘aware client
#    - Claudeâ€¯Code â†’ /mcp â†’ â€œshadcnâ€‘ideâ€ â†’ Connect
#    - Cursor â†’ Settings â†’ MCP â†’ add http://localhost:4000/mcp
#    - VSâ€¯Code â†’ .vscode/mcp.json (copy the root .mcp.json) â†’ Start
```

You now have:

* **A blackâ€‘themed, responsive IDE** (pure `#000000` background).  
* **MCP integration** ready for any client that supports the Modelâ€‘Contextâ€‘Protocol.  
* **Three LLM providers** (Codestral, ChatGPTâ€‘OSS, dKimi) selectable via UI dropdowns.  
* **All developerâ€‘experience niceties** (hotkeys, toast feedback, live fileâ€‘tree, Git panel).  
* **A simple Dockerâ€‘Compose stack** that is *the* recommended way to run it locally â€“ **Firecracker is unnecessary** unless you go multiâ€‘tenant.

If you ever decide to switch to Firecracker for a SaaS offering, you only need to wrap the same Docker image inside a Firecracker microâ€‘VM (the entrypoint stays exactly the same), so the migration is trivial.

--- 

### ğŸ‰ Youâ€™re all set!  
Run the steps above, open the UI, toggle the model dropdowns, and start typing â€“ the AI will respond instantly, and any MCPâ€‘compatible assistant will be able to call the same tools automatically.  

Feel free to ask for any additional polish (e.g., **download workspace as zip**, **share a readâ€‘only URL**, **add a languageâ€‘server**), and Iâ€™ll give you the exact diff. Happy hacking! ğŸš€